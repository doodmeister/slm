# Default configuration for SLM training and generation
# This file provides a comprehensive template for all settings

model:
  model_type: "rnn"  # Options: "rnn", "transformer"
  
  # RNN-specific parameters
  rnn_type: "lstm"  # Options: "lstm", "gru", "rnn"
  embedding_dim: 128
  hidden_dim: 256
  num_layers: 2
  dropout: 0.3
  tie_weights: false
  
  # Transformer-specific parameters
  d_model: 256
  n_heads: 8
  n_layers_transformer: 6
  d_ff: 1024
  max_len: 1000

training:
  # Basic training parameters
  seq_length: 100
  batch_size: 64
  epochs: 10
  learning_rate: 0.002
  weight_decay: 0.0
  
  # Optimizer settings
  optimizer: "adam"  # Options: "adam", "sgd", "rmsprop", "adamw"
  momentum: 0.9  # For SGD
  
  # Learning rate scheduling
  scheduler: "none"  # Options: "none", "step", "cosine", "exponential", "reduce_on_plateau"
  step_size: 10
  gamma: 0.1
  
  # Regularization
  grad_clip: 1.0
  
  # Early stopping
  early_stopping: true
  patience: 5
  min_delta: 0.001
  
  # Checkpointing
  save_every: 1
  save_best_only: false
  
  # Validation
  validation_split: 0.1
  
  # Performance
  device: null  # Auto-detect if null
  num_workers: 0
  pin_memory: true

generation:
  # Generation parameters
  length: 200
  temperature: 1.0
  top_k: 0  # 0 = disabled
  top_p: 0.9
  sampling_method: "temperature"  # Options: "greedy", "temperature", "top_k", "top_p", "nucleus"
  
  # Advanced parameters
  seed: null
  max_new_tokens: null
  repetition_penalty: 1.0
  length_penalty: 1.0

# Paths
data_path: "data.txt"
checkpoint_dir: "checkpoints"
log_dir: "logs"

# Logging
log_level: "INFO"
log_to_file: true

# Reproducibility
seed: 42
deterministic: false
