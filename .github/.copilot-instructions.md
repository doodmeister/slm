# Copilot Instructions for SLM Project

## Project Overview
This is a character-level language model project built with PyTorch, supporting both RNN-based (LSTM/GRU/RNN) and Transformer-based architectures with multi-head attention. The project provides flexible model selection and comprehensive training/generation options.

## Project Structure
- `core/model.py` - Contains CharRNN and CharTransformer classes
- `core/train.py` - Training script supporting both RNN and Transformer models
- `core/generate.py` - Text generation with multiple sampling strategies
- `gui_enhanced.py` - Comprehensive GUI with model type selection
- `gui.py` - Simple GUI (legacy compatibility)
- `requirements.txt` - Dependencies (torch, matplotlib, transformers, numpy)
- `checkpoints/` - Directory for saved model checkpoints

## Model Architectures

### CharTransformer
- Multi-head self-attention with configurable heads (1-16)
- Positional encoding using sinusoidal embeddings  
- Pre-layer normalization (Pre-LN) architecture
- Causal masking for autoregressive generation
- Feed-forward networks with ReLU activation
- Configurable model dimension (d_model), layers, and FFN size

### CharRNN (Enhanced)
- Support for LSTM, GRU, and vanilla RNN
- Configurable embedding and hidden dimensions
- Multi-layer support with dropout
- Optional weight tying between input/output embeddings
- Proper hidden state management

## Key Features
- **Dual Architecture Support**: Choose between RNN and Transformer models
- **Multi-Head Attention**: Transformer models with scaled dot-product attention
- **Advanced Training**: Early stopping, learning rate scheduling, gradient clipping
- **Multiple Sampling**: Temperature, top-k, top-p, and greedy generation
- **Comprehensive GUI**: Model type selection with appropriate parameter controls

## Model Configuration

### Transformer Parameters
- `model_type`: 'transformer'
- `d_model`: Model dimension (128-1024, default: 256)
- `n_heads`: Number of attention heads (1-16, default: 8)  
- `n_layers`: Transformer layers (1-12, default: 6)
- `d_ff`: Feed-forward dimension (256-4096, default: 1024)
- `max_len`: Maximum sequence length (100-2000, default: 1000)
- `dropout`: Dropout rate (0.0-0.8, default: 0.1)

### RNN Parameters
- `model_type`: 'rnn' (default)
- `rnn_type`: 'lstm', 'gru', 'rnn'
- `embedding_dim`: Character embedding dimension
- `hidden_dim`: RNN hidden state dimension
- `num_layers`: Number of RNN layers
- `dropout`: Dropout rate
- `tie_weights`: Boolean for weight tying

## Code Generation Guidelines

### Model Creation
```python
# Transformer model
model_config = {
    'model_type': 'transformer',
    'd_model': 256,
    'n_heads': 8,
    'n_layers': 6
}
model = CharTransformer(vocab_size, **model_config)

# RNN model  
model_config = {
    'model_type': 'rnn',
    'rnn_type': 'lstm',
    'hidden_dim': 256
}
model = CharRNN(vocab_size, **model_config)
```

### Training Patterns
- Use unified `train_model()` function for both architectures
- Automatic model type detection from config
- Different forward pass handling (RNN vs Transformer)
- Proper checkpoint saving with model configuration

### Generation Patterns
- Transformer: Pass full sequence context each step
- RNN: Maintain hidden state across generation steps
- Use `isinstance(model, CharTransformer)` for model type detection
- Handle different input requirements appropriately

## Attention Mechanism Details
- **Scaled Dot-Product**: `Attention(Q,K,V) = softmax(QK^T/√d_k)V`
- **Multi-Head**: Multiple parallel attention computations
- **Causal Masking**: Lower triangular mask for autoregressive generation
- **Positional Encoding**: Sinusoidal position embeddings

## Memory Considerations
- Transformers: O(n²) memory due to attention matrix
- RNNs: O(n) memory with sequential processing
- Consider sequence length limits for transformers
- Batch size adjustments based on available memory

## Performance Characteristics
- **Transformers**: Better long-range dependencies, higher memory usage
- **RNNs**: More memory efficient, good local dependencies
- **Training Speed**: RNNs faster per epoch, Transformers better parallelization
- **Generation**: Transformers more coherent, RNNs more efficient

## GUI Integration
- Model type selection updates available parameters
- Dynamic widget enabling/disabling based on architecture
- Appropriate default values for each model type
- Real-time parameter validation

## Error Handling Patterns
- Model type validation in training/generation
- Proper device handling for both architectures  
- Checkpoint compatibility checking
- Memory overflow prevention

## Best Practices
- Start with smaller transformer models and scale up
- Use gradient clipping for both architectures (default: 1.0)
- Monitor memory usage with transformers
- Save model configuration in checkpoints
- Use appropriate sampling methods for generation quality
